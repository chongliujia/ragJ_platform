"""
å·¥ä½œæµå¹¶è¡Œæ‰§è¡Œæ¼”ç¤º
å±•ç¤ºä¸²è¡Œæ‰§è¡Œä¸å¹¶è¡Œæ‰§è¡Œçš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
from datetime import datetime
from typing import Dict, Any

from app.schemas.workflow import (
    WorkflowDefinition,
    WorkflowNode,
    WorkflowEdge,
    NodeFunctionSignature,
    NodeInputSchema,
    NodeOutputSchema,
    DataType
)
from app.services.workflow_execution_engine import workflow_execution_engine
from app.services.workflow_parallel_executor import workflow_parallel_executor


def create_complex_workflow() -> WorkflowDefinition:
    """åˆ›å»ºå¤æ‚çš„å·¥ä½œæµç”¨äºæ¼”ç¤º"""
    
    # å®šä¹‰èŠ‚ç‚¹
    nodes = [
        # è¾“å…¥èŠ‚ç‚¹
        WorkflowNode(
            id="input_1",
            type="input",
            name="æ•°æ®è¾“å…¥",
            description="æ¥æ”¶è¾“å…¥æ•°æ®",
            function_signature=NodeFunctionSignature(
                name="input_data",
                description="è¾“å…¥æ•°æ®",
                category="io",
                inputs=[],
                outputs=[
                    NodeOutputSchema(
                        name="data",
                        type=DataType.OBJECT,
                        description="è¾“å…¥æ•°æ®",
                        required=True
                    )
                ]
            ),
            position={"x": 100, "y": 100}
        ),
        
        # å¹¶è¡Œå¤„ç†åˆ†æ”¯A
        WorkflowNode(
            id="rag_a",
            type="rag_retriever",
            name="çŸ¥è¯†æ£€ç´¢A",
            description="ä»çŸ¥è¯†åº“Aæ£€ç´¢ç›¸å…³ä¿¡æ¯",
            function_signature=NodeFunctionSignature(
                name="rag_retrieve",
                description="çŸ¥è¯†æ£€ç´¢",
                category="data",
                inputs=[
                    NodeInputSchema(
                        name="query",
                        type=DataType.STRING,
                        description="æŸ¥è¯¢æ–‡æœ¬",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="documents",
                        type=DataType.ARRAY,
                        description="æ£€ç´¢åˆ°çš„æ–‡æ¡£",
                        required=True
                    )
                ]
            ),
            config={
                "knowledge_base": "knowledge_a",
                "top_k": 5,
                "cpu_intensive": False,
                "memory_intensive": False
            },
            position={"x": 300, "y": 50}
        ),
        
        # å¹¶è¡Œå¤„ç†åˆ†æ”¯B
        WorkflowNode(
            id="rag_b",
            type="rag_retriever",
            name="çŸ¥è¯†æ£€ç´¢B",
            description="ä»çŸ¥è¯†åº“Bæ£€ç´¢ç›¸å…³ä¿¡æ¯",
            function_signature=NodeFunctionSignature(
                name="rag_retrieve",
                description="çŸ¥è¯†æ£€ç´¢",
                category="data",
                inputs=[
                    NodeInputSchema(
                        name="query",
                        type=DataType.STRING,
                        description="æŸ¥è¯¢æ–‡æœ¬",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="documents",
                        type=DataType.ARRAY,
                        description="æ£€ç´¢åˆ°çš„æ–‡æ¡£",
                        required=True
                    )
                ]
            ),
            config={
                "knowledge_base": "knowledge_b",
                "top_k": 5,
                "cpu_intensive": False,
                "memory_intensive": False
            },
            position={"x": 300, "y": 150}
        ),
        
        # å¹¶è¡Œå¤„ç†åˆ†æ”¯C
        WorkflowNode(
            id="classifier_1",
            type="classifier",
            name="æ–‡æœ¬åˆ†ç±»å™¨",
            description="å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†ç±»",
            function_signature=NodeFunctionSignature(
                name="classify_text",
                description="æ–‡æœ¬åˆ†ç±»",
                category="ai",
                inputs=[
                    NodeInputSchema(
                        name="text",
                        type=DataType.STRING,
                        description="å¾…åˆ†ç±»æ–‡æœ¬",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="class",
                        type=DataType.STRING,
                        description="åˆ†ç±»ç»“æœ",
                        required=True
                    )
                ]
            ),
            config={
                "classes": ["æŠ€æœ¯", "ä¸šåŠ¡", "ç®¡ç†"],
                "model": "qwen-turbo"
            },
            position={"x": 300, "y": 250}
        ),
        
        # æ•°æ®åˆå¹¶èŠ‚ç‚¹
        WorkflowNode(
            id="merger_1",
            type="data_transformer",
            name="æ•°æ®åˆå¹¶",
            description="åˆå¹¶å¤šä¸ªæ•°æ®æºçš„ç»“æœ",
            function_signature=NodeFunctionSignature(
                name="merge_data",
                description="æ•°æ®åˆå¹¶",
                category="processing",
                inputs=[
                    NodeInputSchema(
                        name="data_a",
                        type=DataType.ARRAY,
                        description="æ•°æ®æºA",
                        required=True
                    ),
                    NodeInputSchema(
                        name="data_b",
                        type=DataType.ARRAY,
                        description="æ•°æ®æºB",
                        required=True
                    ),
                    NodeInputSchema(
                        name="class_info",
                        type=DataType.STRING,
                        description="åˆ†ç±»ä¿¡æ¯",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="merged_data",
                        type=DataType.OBJECT,
                        description="åˆå¹¶åçš„æ•°æ®",
                        required=True
                    )
                ]
            ),
            config={
                "merge_strategy": "weighted_average"
            },
            position={"x": 600, "y": 150}
        ),
        
        # LLMå¤„ç†èŠ‚ç‚¹
        WorkflowNode(
            id="llm_1",
            type="llm",
            name="æ™ºèƒ½ç”Ÿæˆ",
            description="åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›å¤",
            function_signature=NodeFunctionSignature(
                name="llm_generate",
                description="æ™ºèƒ½ç”Ÿæˆ",
                category="llm",
                inputs=[
                    NodeInputSchema(
                        name="context",
                        type=DataType.OBJECT,
                        description="ä¸Šä¸‹æ–‡æ•°æ®",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="content",
                        type=DataType.STRING,
                        description="ç”Ÿæˆçš„å†…å®¹",
                        required=True
                    )
                ]
            ),
            config={
                "model": "qwen-turbo",
                "temperature": 0.7,
                "max_tokens": 1000,
                "cpu_intensive": True,
                "memory_intensive": True
            },
            position={"x": 900, "y": 150}
        ),
        
        # è¾“å‡ºèŠ‚ç‚¹
        WorkflowNode(
            id="output_1",
            type="output",
            name="ç»“æœè¾“å‡º",
            description="è¾“å‡ºæœ€ç»ˆç»“æœ",
            function_signature=NodeFunctionSignature(
                name="output_data",
                description="è¾“å‡ºæ•°æ®",
                category="io",
                inputs=[
                    NodeInputSchema(
                        name="data",
                        type=DataType.OBJECT,
                        description="è¾“å‡ºæ•°æ®",
                        required=True
                    )
                ],
                outputs=[
                    NodeOutputSchema(
                        name="result",
                        type=DataType.OBJECT,
                        description="æœ€ç»ˆç»“æœ",
                        required=True
                    )
                ]
            ),
            position={"x": 1200, "y": 150}
        )
    ]
    
    # å®šä¹‰è¾¹
    edges = [
        # ä»è¾“å…¥åˆ°ä¸‰ä¸ªå¹¶è¡Œåˆ†æ”¯
        WorkflowEdge(
            id="edge_1",
            source="input_1",
            target="rag_a",
            source_output="data",
            target_input="query",
            transform="value.get('query', 'default query')"
        ),
        WorkflowEdge(
            id="edge_2",
            source="input_1",
            target="rag_b",
            source_output="data",
            target_input="query",
            transform="value.get('query', 'default query')"
        ),
        WorkflowEdge(
            id="edge_3",
            source="input_1",
            target="classifier_1",
            source_output="data",
            target_input="text",
            transform="value.get('text', 'default text')"
        ),
        
        # ä»ä¸‰ä¸ªåˆ†æ”¯åˆ°åˆå¹¶èŠ‚ç‚¹
        WorkflowEdge(
            id="edge_4",
            source="rag_a",
            target="merger_1",
            source_output="documents",
            target_input="data_a"
        ),
        WorkflowEdge(
            id="edge_5",
            source="rag_b",
            target="merger_1",
            source_output="documents",
            target_input="data_b"
        ),
        WorkflowEdge(
            id="edge_6",
            source="classifier_1",
            target="merger_1",
            source_output="class",
            target_input="class_info"
        ),
        
        # ä»åˆå¹¶åˆ°LLM
        WorkflowEdge(
            id="edge_7",
            source="merger_1",
            target="llm_1",
            source_output="merged_data",
            target_input="context"
        ),
        
        # ä»LLMåˆ°è¾“å‡º
        WorkflowEdge(
            id="edge_8",
            source="llm_1",
            target="output_1",
            source_output="content",
            target_input="data"
        )
    ]
    
    # åˆ›å»ºå·¥ä½œæµå®šä¹‰
    workflow_definition = WorkflowDefinition(
        id="parallel_demo_workflow",
        name="å¹¶è¡Œæ‰§è¡Œæ¼”ç¤ºå·¥ä½œæµ",
        description="å±•ç¤ºä¸²è¡Œæ‰§è¡Œä¸å¹¶è¡Œæ‰§è¡Œçš„æ€§èƒ½å·®å¼‚",
        version="1.0.0",
        nodes=nodes,
        edges=edges,
        global_config={
            "timeout": 60,
            "enable_parallel_execution": True,
            "max_parallel_workers": 5
        }
    )
    
    return workflow_definition


async def benchmark_execution_modes():
    """å¯¹æ¯”ä¸²è¡Œä¸å¹¶è¡Œæ‰§è¡Œæ€§èƒ½"""
    
    print("ğŸš€ å·¥ä½œæµå¹¶è¡Œæ‰§è¡Œæ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow_def = create_complex_workflow()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    input_data = {
        "query": "å¦‚ä½•æé«˜ç³»ç»Ÿæ€§èƒ½ï¼Ÿ",
        "text": "æˆ‘ä»¬éœ€è¦ä¼˜åŒ–ç³»ç»Ÿæ¶æ„ä»¥æé«˜æ€§èƒ½å’Œç¨³å®šæ€§",
        "user_id": "demo_user"
    }
    
    # æµ‹è¯•1: ä¸²è¡Œæ‰§è¡Œ
    print("\nğŸ“Š æµ‹è¯•1: ä¸²è¡Œæ‰§è¡Œ")
    print("-" * 40)
    
    serial_start = time.time()
    
    try:
        serial_context = await workflow_execution_engine.execute_workflow(
            workflow_definition=workflow_def,
            input_data=input_data,
            debug=True,
            enable_parallel=False
        )
        
        serial_duration = time.time() - serial_start
        
        print(f"âœ… ä¸²è¡Œæ‰§è¡Œå®Œæˆ")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {serial_duration:.2f}ç§’")
        print(f"ğŸ“Š çŠ¶æ€: {serial_context.status}")
        print(f"ğŸ”„ æ­¥éª¤æ•°é‡: {len(serial_context.steps)}")
        
        # æ˜¾ç¤ºæ­¥éª¤è¯¦æƒ…
        print(f"\nğŸ“ æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(serial_context.steps, 1):
            print(f"{i}. {step.node_name}: {step.duration:.3f}ç§’ ({step.status})")
        
    except Exception as e:
        print(f"âŒ ä¸²è¡Œæ‰§è¡Œå¤±è´¥: {str(e)}")
        serial_duration = time.time() - serial_start
        serial_context = None
    
    # æµ‹è¯•2: å¹¶è¡Œæ‰§è¡Œ
    print(f"\nğŸ“Š æµ‹è¯•2: å¹¶è¡Œæ‰§è¡Œ")
    print("-" * 40)
    
    parallel_start = time.time()
    
    try:
        parallel_context = await workflow_execution_engine.execute_workflow(
            workflow_definition=workflow_def,
            input_data=input_data,
            debug=True,
            enable_parallel=True
        )
        
        parallel_duration = time.time() - parallel_start
        
        print(f"âœ… å¹¶è¡Œæ‰§è¡Œå®Œæˆ")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {parallel_duration:.2f}ç§’")
        print(f"ğŸ“Š çŠ¶æ€: {parallel_context.status}")
        print(f"ğŸ”„ æ­¥éª¤æ•°é‡: {len(parallel_context.steps)}")
        
        # æ˜¾ç¤ºæ­¥éª¤è¯¦æƒ…
        print(f"\nğŸ“ æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(parallel_context.steps, 1):
            print(f"{i}. {step.node_name}: {step.duration:.3f}ç§’ ({step.status})")
        
    except Exception as e:
        print(f"âŒ å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {str(e)}")
        parallel_duration = time.time() - parallel_start
        parallel_context = None
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ† æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    
    if serial_context and parallel_context:
        speedup = serial_duration / parallel_duration
        improvement = (serial_duration - parallel_duration) / serial_duration * 100
        
        print(f"ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_duration:.2f}ç§’")
        print(f"å¹¶è¡Œæ‰§è¡Œæ—¶é—´: {parallel_duration:.2f}ç§’")
        print(f"æ€§èƒ½æå‡: {speedup:.2f}å€")
        print(f"æ—¶é—´èŠ‚çœ: {improvement:.1f}%")
        
        if speedup > 1.5:
            print("ğŸ‰ å¹¶è¡Œæ‰§è¡Œæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼")
        elif speedup > 1.1:
            print("âœ… å¹¶è¡Œæ‰§è¡Œæœ‰ä¸€å®šçš„æ€§èƒ½æå‡")
        else:
            print("âš ï¸  å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æå‡ä¸æ˜æ˜¾")
    
    return serial_context, parallel_context


async def demonstrate_resource_management():
    """æ¼”ç¤ºèµ„æºç®¡ç†åŠŸèƒ½"""
    
    print("\nğŸ”§ èµ„æºç®¡ç†æ¼”ç¤º")
    print("=" * 60)
    
    # é…ç½®èµ„æºæ± 
    workflow_execution_engine.configure_parallel_execution(
        enable=True,
        max_workers=5,
        total_cpu=4.0,
        total_memory=4096,
        total_network=500
    )
    
    # è·å–èµ„æºåˆ©ç”¨ç‡
    stats = workflow_execution_engine.get_parallel_statistics()
    
    print("ğŸ“Š èµ„æºé…ç½®:")
    if stats.get("parallel_execution_enabled"):
        resource_util = stats.get("resource_utilization", {})
        print(f"   CPUåˆ©ç”¨ç‡: {resource_util.get('cpu', 0)*100:.1f}%")
        print(f"   å†…å­˜åˆ©ç”¨ç‡: {resource_util.get('memory', 0)*100:.1f}%")
        print(f"   ç½‘ç»œåˆ©ç”¨ç‡: {resource_util.get('network', 0)*100:.1f}%")
    else:
        print("   å¹¶è¡Œæ‰§è¡Œæœªå¯ç”¨")
    
    # æ˜¾ç¤ºèŠ‚ç‚¹æ€§èƒ½ç»Ÿè®¡
    if "node_performance" in stats:
        print(f"\nğŸ“ˆ èŠ‚ç‚¹æ€§èƒ½ç»Ÿè®¡:")
        for node_id, perf in stats["node_performance"].items():
            print(f"   {node_id}:")
            print(f"     å¹³å‡æ‰§è¡Œæ—¶é—´: {perf.get('avg_duration', 0):.3f}ç§’")
            print(f"     æ‰§è¡Œæ¬¡æ•°: {perf.get('execution_count', 0)}")
            print(f"     æœ€å¿«æ—¶é—´: {perf.get('min_duration', 0):.3f}ç§’")
            print(f"     æœ€æ…¢æ—¶é—´: {perf.get('max_duration', 0):.3f}ç§’")


async def demonstrate_optimization_analysis():
    """æ¼”ç¤ºä¼˜åŒ–åˆ†æåŠŸèƒ½"""
    
    print("\nğŸ” å·¥ä½œæµä¼˜åŒ–åˆ†æ")
    print("=" * 60)
    
    workflow_def = create_complex_workflow()
    
    # æ¨¡æ‹Ÿä¼˜åŒ–åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("ğŸ“Š å¹¶è¡ŒåŒ–æ½œåŠ›åˆ†æ:")
    print(f"   èŠ‚ç‚¹æ€»æ•°: {len(workflow_def.nodes)}")
    print(f"   è¾¹æ•°é‡: {len(workflow_def.edges)}")
    
    # åˆ†æä¾èµ–å…³ç³»
    dependencies = {}
    for node in workflow_def.nodes:
        dependencies[node.id] = []
    
    for edge in workflow_def.edges:
        dependencies[edge.target].append(edge.source)
    
    # è®¡ç®—å¯å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹
    parallel_groups = []
    processed = set()
    
    def find_parallel_nodes(current_nodes):
        if not current_nodes:
            return []
        
        # æ‰¾åˆ°æ²¡æœ‰ä¾èµ–æˆ–ä¾èµ–å·²å¤„ç†çš„èŠ‚ç‚¹
        ready_nodes = []
        for node_id in current_nodes:
            if all(dep in processed for dep in dependencies[node_id]):
                ready_nodes.append(node_id)
        
        return ready_nodes
    
    remaining_nodes = set(node.id for node in workflow_def.nodes)
    
    level = 1
    while remaining_nodes:
        ready_nodes = find_parallel_nodes(remaining_nodes)
        if not ready_nodes:
            break
        
        print(f"   ç¬¬{level}å±‚å¹¶è¡ŒèŠ‚ç‚¹: {len(ready_nodes)}ä¸ª")
        for node_id in ready_nodes:
            processed.add(node_id)
            remaining_nodes.remove(node_id)
        
        parallel_groups.append(ready_nodes)
        level += 1
    
    max_parallel = max(len(group) for group in parallel_groups) if parallel_groups else 0
    print(f"   æœ€å¤§å¹¶è¡Œåº¦: {max_parallel}")
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if max_parallel > 1:
        print("   âœ… å·¥ä½œæµå…·æœ‰è‰¯å¥½çš„å¹¶è¡ŒåŒ–æ½œåŠ›")
        print("   ğŸ“ˆ å»ºè®®å¯ç”¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼")
    else:
        print("   âš ï¸  å·¥ä½œæµä¸»è¦ä¸ºä¸²è¡Œç»“æ„")
        print("   ğŸ”„ è€ƒè™‘é‡æ„ä»¥å¢åŠ å¹¶è¡Œåº¦")
    
    if len(workflow_def.nodes) > 5:
        print("   ğŸ¯ å»ºè®®é…ç½®è¶³å¤Ÿçš„å·¥ä½œçº¿ç¨‹")
        print("   ğŸ’¾ è€ƒè™‘å¯ç”¨ç»“æœç¼“å­˜")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    
    print("ğŸ¯ å·¥ä½œæµå¹¶è¡Œæ‰§è¡Œå®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # 1. æ‰§è¡Œæ€§èƒ½å¯¹æ¯”
    serial_context, parallel_context = await benchmark_execution_modes()
    
    # 2. èµ„æºç®¡ç†æ¼”ç¤º
    await demonstrate_resource_management()
    
    # 3. ä¼˜åŒ–åˆ†ææ¼”ç¤º
    await demonstrate_optimization_analysis()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    
    # æ¸…ç†èµ„æº
    workflow_execution_engine.reset_parallel_cache()


if __name__ == "__main__":
    asyncio.run(main())